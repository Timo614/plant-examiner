{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105267,"databundleVersionId":12693789,"sourceType":"competition"},{"sourceId":12512021,"sourceType":"datasetVersion","datasetId":7897384}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Plant Disease Dataset Preparation for SFT Training\n\nI have previously trained [TinyML models for plant disease detection](https://www.kaggle.com/code/timothylovett/plant-disease-shrunken-efficientnet) so I wanted to approach this hackathon with that in mind as I felt the Gemma 3N model could be a good solution to providing a holistic approach with proper information to help not just identify but to solve any issues arise from the diseases.\n\nThe problem I faced, of course, is that the dataset itself is not structured for SFT Training. To properly train the model I'd need a set of text for each image such that the model could then generate similar text during execution. To solve this I decided I'd leverage the model itself to first generate text as if it were seeing that image (this notebook) and then use that for training itself. I opted to adjust the temperature slightly to give more variety with the resulting text and I then proceeded to associate the dataset with these printouts. To further improve the training I also have the logic generate the text in multiple languages (a subset of the 140 supported) to avoid issues where I may cause catastrophic loss of the model's multi language functionality. I'm then randomly selecting the text and language when processing the dataset.\n\nThank you to Unsloth/Daniel for providing [their notebook](https://www.kaggle.com/code/danielhanchen/gemma-3n-4b-multimodal-finetuning-inference) as part of the competition materials as I was able to get up and running on kaggle very quickly as a result.\n\nAdditionally thank you to the [PlantVillage dataset](https://github.com/spMohanty/PlantVillage-Dataset) for providing images used for this training.\n\n## Future\n\nThis project utilizes only a subset of diseases but showcases a training approach that persists multi language support and uses the existing model to generate its own captions used for fine tuning. It would be useful for a future project to document additional plant diseases in such a way that a larger corpus of data could be relied on. The more data we can gather the better we can help each other as we navigate through issues such as diseases and their solutions. \n\n## Citation\n\n@article{Mohanty_Hughes_Salathé_2016,\ntitle={Using deep learning for image-based plant disease detection},\nvolume={7},\nDOI={10.3389/fpls.2016.01419},\njournal={Frontiers in Plant Science},\nauthor={Mohanty, Sharada P. and Hughes, David P. and Salathé, Marcel},\nyear={2016},\nmonth={Sep}} ","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:33:24.636059Z","iopub.execute_input":"2025-07-19T00:33:24.636766Z","iopub.status.idle":"2025-07-19T00:33:39.230721Z","shell.execute_reply.started":"2025-07-19T00:33:24.636725Z","shell.execute_reply":"2025-07-19T00:33:39.229754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Install latest transformers for Gemma 3N\n!pip install --no-deps git+https://github.com/huggingface/transformers.git # Only for Gemma 3N\n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:33:39.232389Z","iopub.execute_input":"2025-07-19T00:33:39.232969Z","iopub.status.idle":"2025-07-19T00:34:15.437621Z","shell.execute_reply.started":"2025-07-19T00:33:39.232941Z","shell.execute_reply":"2025-07-19T00:34:15.436694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/spMohanty/PlantVillage-Dataset.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:34:15.438899Z","iopub.execute_input":"2025-07-19T00:34:15.439447Z","iopub.status.idle":"2025-07-19T00:34:15.443353Z","shell.execute_reply.started":"2025-07-19T00:34:15.439409Z","shell.execute_reply":"2025-07-19T00:34:15.442665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n    # Pretrained models\n    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n\n    # Other Gemma 3 quants\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3n-E4B-it\", # Or \"unsloth/gemma-3n-E2B-it\"\n    dtype = None, # None for auto detection\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:34:15.445546Z","iopub.execute_input":"2025-07-19T00:34:15.445737Z","iopub.status.idle":"2025-07-19T00:36:13.126267Z","shell.execute_reply.started":"2025-07-19T00:34:15.445721Z","shell.execute_reply":"2025-07-19T00:36:13.125677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\nimport gc\n# Helper function for inference\ndef do_gemma_3n_inference(model, tokenizer, messages, max_new_tokens=128):\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n    ).to(\"cuda\")\n\n    # generate returns the full sequence of IDs because we dropped `streamer=…`\n    out_ids = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=1.15,\n        top_p=0.95,\n        top_k=64,\n    )\n\n    # slice off the prompt part and decode\n    gen_ids = out_ids[0][inputs[\"input_ids\"].shape[-1]:]\n    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n\n    del inputs\n    torch.cuda.empty_cache()\n    gc.collect()\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:36:13.127085Z","iopub.execute_input":"2025-07-19T00:36:13.127363Z","iopub.status.idle":"2025-07-19T00:36:13.133641Z","shell.execute_reply.started":"2025-07-19T00:36:13.127336Z","shell.execute_reply":"2025-07-19T00:36:13.132899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For each plant / disease combo I'm generating 10 text outputs. I then translate the text to other languages and randomly select the text during training.","metadata":{}},{"cell_type":"code","source":"import csv, re, time\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# LANGUAGES=[\"English\",\"Chinese\",\"Hindi\",\"Spanish\",\"Arabic\",\"French\",\"Bengali\",\"Portuguese\",\"Russian\",\"Indonesian\",\"Urdu\",\"German\",\"Japanese\",\"Vietnamese\",\"Turkish\",\"Swahili\",\"Tagalog\",\"Korean\",\"Thai\",\"Italian\",\"Hebrew\"]\n\nPLANT_STATE_RAW=[\"Apple___Apple_scab\",\"Apple___Black_rot\",\"Apple___Cedar_apple_rust\",\"Apple___healthy\",\"Blueberry___healthy\",\"Cherry___Powdery_mildew\",\"Cherry___healthy\",\"Corn_(maize)___Cercospora_leaf_spot\",\"Corn_(maize)___Common_rust_\",\"Corn_(maize)___Northern_Leaf_Blight\",\"Corn_(maize)___healthy\",\"Grape___Black_rot\",\"Grape___Esca_(Black_Measles)\",\"Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\",\"Grape___healthy\",\"Orange___Haunglongbing_(Citrus_greening)\",\"Peach___Bacterial_spot\",\"Peach___healthy\",\"Pepper,_bell___Bacterial_spot\",\"Pepper,_bell___healthy\",\"Potato___Early_blight\",\"Potato___Late_blight\",\"Potato___healthy\",\"Raspberry___healthy\",\"Soybean___healthy\",\"Squash___Powdery_mildew\",\"Strawberry___Leaf_scorch\",\"Strawberry___healthy\",\"Tomato___Bacterial_spot\",\"Tomato___Early_blight\",\"Tomato___Late_blight\",\"Tomato___Leaf_Mold\",\"Tomato___Septoria_leaf_spot\",\"Tomato___Spider_mites Two-spotted_spider_mite\",\"Tomato___Target_Spot\",\"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",\"Tomato___Tomato_mosaic_virus\",\"Tomato___healthy\"]\n\nSAMPLES_PER_LANGUAGE=5\nCSV_PATH=Path(\"plant_state_descriptions.csv\")\nSLEEP_BETWEEN_CALLS=0.05\n\ndef _clean(t):\n    if t.startswith(\"Pepper,_bell\"):\n        return \"Bell Pepper\"\n    t=t.replace(\"_\",\" \").replace(\"  \",\" \").strip()\n    t=re.sub(r\"\\(([^)]+)\\)\",lambda m:f\"({m.group(1).title()})\",t)\n    return t.title()\n\ndef normalise_pair(r):\n    a,b=r.split(\"___\",1)\n    return _clean(a),_clean(b)\n\ntotal=len(PLANT_STATE_RAW)*SAMPLES_PER_LANGUAGE\nwith CSV_PATH.open(\"w\",newline=\"\",encoding=\"utf-8\") as f:\n    w=csv.writer(f)\n    w.writerow([\"Plant\",\"State\",\"Language\",\"Text\"])\n    with tqdm(total=total,desc=\"Rows\") as pbar:\n        for raw in PLANT_STATE_RAW:\n            plant,state=normalise_pair(raw)\n            prompt=(f\"Plant: {plant}, State: {state} - \"\n                    \"name and describe state with information (without explicitly \"\n                    \"mentioning it's a detailed description)\")\n            for _ in range(SAMPLES_PER_LANGUAGE):\n                resp=do_gemma_3n_inference(model,tokenizer,[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],max_new_tokens=512)\n                resp=resp if isinstance(resp,str) else str(resp)\n                w.writerow([plant,state,\"English\",resp])\n                pbar.update(1)\n                time.sleep(SLEEP_BETWEEN_CALLS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T00:42:21.976346Z","iopub.execute_input":"2025-07-19T00:42:21.977048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}